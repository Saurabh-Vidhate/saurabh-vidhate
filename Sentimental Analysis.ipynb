{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eefcc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e1b51c8",
   "metadata": {},
   "source": [
    "# Imporrting Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a1a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing and Data manipulation\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# Libraries and packages for NLP\n",
    "import nltk\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ec59f",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662c6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "def clean_text(text):\n",
    "    # Remove special characters, digits, and convert to lowercase\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = text.split()  # Split by whitespace\n",
    "    return tokens\n",
    "\n",
    "def load_data(data_dir, subset):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        label_dir = os.path.join(data_dir, subset, label)\n",
    "        for filename in os.listdir(label_dir):\n",
    "            with open(os.path.join(label_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                review = file.read()\n",
    "                cleaned_review = clean_text(review)\n",
    "                tokens = tokenize_text(cleaned_review)\n",
    "                reviews.append(tokens)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return reviews, labels\n",
    "\n",
    "data_dir = r'F:\\Datasets\\Sentimental data\\IMDB_reviews'  # Update this to your dataset directory\n",
    "train_reviews, train_labels = load_data(data_dir, 'train')  \n",
    "test_reviews, test_labels = load_data(data_dir, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80472467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67e6750d",
   "metadata": {},
   "source": [
    "# BoW feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d89c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Load BoW features and convert to a sparse matrix\n",
    "def load_sparse_feat_file(file_path, num_features):\n",
    "    num_samples = sum(1 for line in open(file_path))\n",
    "    X = lil_matrix((num_samples, num_features), dtype=int)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as feat_file:\n",
    "        for row_idx, line in enumerate(feat_file):\n",
    "            features = line.strip().split(' ')\n",
    "            for feature in features[1:]:  # Skip the label\n",
    "                index, count = feature.split(':')\n",
    "                X[row_idx, int(index)] = int(count)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Load vocabulary and get the number of features\n",
    "vocab = []\n",
    "with open('F:/Datasets/Sentimental data/IMDB_reviews/imdb.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    vocab = vocab_file.read().splitlines()\n",
    "num_features = len(vocab)\n",
    "\n",
    "# Load BoW features and labels as sparse matrices\n",
    "X_train_bow = load_sparse_feat_file('F:/Datasets/Sentimental data/IMDB_reviews/train/labeledBow.feat', num_features)\n",
    "X_test_bow = load_sparse_feat_file('F:/Datasets/Sentimental data/IMDB_reviews/test/labeledBow.feat', num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b0f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels  # Implement load_labels function\n",
    "y_test = test_labels    # Implement load_labels function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324f7861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.87      0.87     12500\n",
      "    Positive       0.87      0.86      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_bow, y_train)\n",
    "\n",
    "# Step : Feature Extraction and Classification\n",
    "y_pred = classifier.predict(X_test_bow)\n",
    "\n",
    "# Step : Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdad190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.84292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.84      0.84     12500\n",
      "    Positive       0.84      0.84      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train_bow, train_labels)\n",
    "rf_predictions = rf_classifier.predict(X_test_bow)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "rf_accuracy = accuracy_score(test_labels, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "report = classification_report(y_test,rf_predictions, target_names=['Negative', 'Positive'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc6b797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.76      0.80     12500\n",
      "    Positive       0.79      0.86      0.82     12500\n",
      "\n",
      "    accuracy                           0.81     25000\n",
      "   macro avg       0.82      0.81      0.81     25000\n",
      "weighted avg       0.82      0.81      0.81     25000\n",
      "\n",
      "Gradient Boosting Accuracy: 0.8132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "gb_classifier.fit(X_train_bow, train_labels)\n",
    "gb_predictions = gb_classifier.predict(X_test_bow)\n",
    "\n",
    "# Evaluation\n",
    "gb_accuracy = accuracy_score(test_labels, gb_predictions)\n",
    "report = classification_report(y_test,gb_predictions, target_names=['Negative', 'Positive'])\n",
    "print(report)\n",
    "print(\"Gradient Boosting Accuracy:\", gb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def load_unsup_feat_file(file_path, vocab_size):\n",
    "    feature_matrix = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as feat_file:\n",
    "        for line in feat_file:\n",
    "            features = line.strip().split(' ')\n",
    "            feature_vector = [0] * vocab_size\n",
    "            for feature in features[1:]:  # Skip the label\n",
    "                index, count = feature.split(':')\n",
    "                feature_vector[int(index)] = int(count)\n",
    "            feature_matrix.append(feature_vector)\n",
    "    return feature_matrix\n",
    "\n",
    "X_unsup_bow = load_unsup_feat_file(r'F:\\Datasets\\Sentimental data\\IMDB_reviews\\train\\unsupBow.feat', len(vocab))\n",
    "\n",
    "# Load labeled BoW features and labels for train and test data (similar to previous steps)\n",
    "X_train_bow = load_feat_file(r'F:/Datasets/Sentimental data/IMDB_reviews/train/labeledBow.feat', len(vocab))\n",
    "X_test_bow = load_feat_file(r'F:/Datasets/Sentimental data/IMDB_reviews/test/labeledBow.feat', len(vocab))\n",
    "y_train = train_labels  # Implement load_labels function\n",
    "y_test = test_labels    # Implement load_labels function\n",
    "\n",
    "# Combine labeled and unsupervised BoW features\n",
    "X_train_combined = np.vstack((X_train_bow, X_unsup_bow))\n",
    "y_train_combined = np.concatenate((y_train, np.zeros(len(X_unsup_bow))))  # Assign unsupervised data to class 0\n",
    "\n",
    "# Train SVM classifier on combined data\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "y_pred = classifier.predict(X_test_bow)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360bb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7658019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a516d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e95207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b5daab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84df4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b4ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ef3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

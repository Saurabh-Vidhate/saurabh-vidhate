Strengths:

High Accuracy: The accuracy of 0.86772 indicates that your model is capable of correctly classifying sentiments for a significant portion of the reviews.

Balanced Precision and Recall: The precision and recall scores for both positive and negative sentiments are relatively balanced. 
This suggests that your model is not heavily biased toward one class and is performing well in identifying both positive and negative sentiments.

Macro-Average F1-Score: While not explicitly mentioned, the macro-average F1-score of 0.87 indicates a reasonably good balance between precision and recall for both classes. 
This implies that your model can effectively capture true positive and true negative instances.

Weaknesses:

F1-Score Interpretation: While the macro-average F1-score is provided, it would be beneficial to see the individual F1-scores for positive and negative classes. 
This would provide a more detailed understanding of your model's performance on each class.

Model Limitations: The reported results are based on the specific dataset and feature extraction technique used (BoW). 
The model's performance might vary when applied to different datasets or if different feature extraction methods, like TF-IDF or word embeddings, are used.

Generalization to New Data: It's important to evaluate your model's performance on unseen data. 
The results you've provided are specific to the test data you used. Performance on new, real-world data could differ due to potential differences in distribution, vocabulary, or sentiment expressions.

Potential Overfitting or Underfitting: Without knowing the complexity of your model and the hyperparameters used, it's challenging to assess whether your model might be overfitting or underfitting the training data.